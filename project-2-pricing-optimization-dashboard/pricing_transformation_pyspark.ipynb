
# Databricks notebook: Pricing Transformation
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lower, trim, round

spark = SparkSession.builder.appName("PricingOptimization").getOrCreate()

# Read datasets from ADLS paths (replace with your storage paths)
competitor_df = spark.read.csv("data/competitor_prices.csv", header=True, inferSchema=True)
catalog_df = spark.read.csv("data/product_catalog.csv", header=True, inferSchema=True)

# Standardize product names
competitor_df = competitor_df.withColumn("Competitor_Product_Name", trim(lower(col("Competitor_Product_Name"))))
catalog_df = catalog_df.withColumn("Product_Name", trim(lower(col("Product_Name"))))

# Join datasets
joined_df = competitor_df.join(
    catalog_df,
    competitor_df.Competitor_Product_Name == catalog_df.Product_Name,
    "inner"
)

# Calculate price gap and percentage difference
final_df = joined_df.withColumn(
    "Price_Gap", col("Competitor_Price") - col("Cost_Price")
).withColumn(
    "Price_Diff_Percent", round(((col("Competitor_Price") - col("Cost_Price")) / col("Cost_Price")) * 100, 2)
)

# Save processed data
final_df.write.mode("overwrite").csv("data/processed_pricing.csv", header=True)

final_df.show()
